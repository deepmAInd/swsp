{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Prediction using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/combined_subjects.csv'\n",
    "data = pd.read_csv(path, index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged amusement\n",
    "First we will merge the amusement data with the baseline data as after the EDA we found out that they are very simmilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_am = data.copy()\n",
    "# baseline = 0\n",
    "data_merged_am[\"label\"] = data_merged_am[\"label\"].replace([1], 0)\n",
    "\n",
    "# stressed = 1\n",
    "data_merged_am[\"label\"] = data_merged_am[\"label\"].replace([2], 1)\n",
    "\n",
    "data_merged_am[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['net_acc_std', 'net_acc_max', 'EDA_tonic_mean', 'EDA_tonic_min', 'EDA_tonic_max']\n",
    "X = data_merged_am[features]\n",
    "y = data_merged_am[\"label\"]\n",
    "\n",
    "X_train_merged_am, X_test_merged_am, y_train_merged_am, y_test_merged_am = train_test_split(X, y, test_size=0.2, random_state=42) # 80% training (1337 samples) and 20% test (334 samples)\n",
    "X_val_merged_am, X_test_merged_am, y_val_merged_am, y_test_merged_am = train_test_split(X_test_merged_am, y_test_merged_am, test_size=0.1, random_state=42) # 90% of test set is used for validation (301 samples) and 10% for testing (33 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to scale the data so it lies between 0 and 1. This is important because the NN algorithm works better with scaled data, as generally activation function use values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_merged_am[features] = scaler.fit_transform(X_train_merged_am[features])\n",
    "X_val_merged_am[features] = scaler.transform(X_val_merged_am[features])\n",
    "X_test_merged_am[features] = scaler.transform(X_test_merged_am[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped amusement\n",
    "First we will drop the amusement data to see if merging it with baseline confuses the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_am = data.copy()\n",
    "\n",
    "# baseline = 0\n",
    "data_no_am[\"label\"] = data_no_am[\"label\"].replace([1], 0)\n",
    "\n",
    "# stressed = 1\n",
    "data_no_am[\"label\"] = data_no_am[\"label\"].replace([2], 1)\n",
    "\n",
    "data_merged_am[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will again split the data into train, validation and test sets and scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_no_am[features]\n",
    "y = data_no_am[\"label\"]\n",
    "\n",
    "X_train_no_am, X_test_no_am, y_train_no_am, y_test_no_am = train_test_split(X, y, test_size=0.2, random_state=42) # 80% training and 20% test\n",
    "X_val_no_am, X_test_no_am, y_val_no_am, y_test_no_am = train_test_split(X_test_no_am, y_test_no_am, test_size=0.1, random_state=42) # 90% of test set is used for validationand 10% for testing\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_no_am[features] = scaler.fit_transform(X_train_no_am[features])\n",
    "X_val_no_am[features] = scaler.transform(X_val_no_am[features])\n",
    "X_test_no_am[features] = scaler.transform(X_test_no_am[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that will create, compile and train a model so we can easily try different models and compare them. We will use the Sequential model from Keras, which is a linear stack of layers. We will add the Dense layers, which are just regular densely connected NN layers. The last layer will have 3 neurons (the number of labels) by default. We will use the relu activation function for the hidden layers and the softmax activation function for the last one. The softmax function is used for multiclass classification problems, it returns the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "def build_model(neurons_per_layer=[64, 64], n_outputs=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(len(neurons_per_layer)):\n",
    "            model.add(Dense(neurons_per_layer[i], activation='relu'))\n",
    "\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged amusement\n",
    "In order to decide for how many epochs we will train the model, we will devide the lenght of the dataset by the batch size and multiply it by two. This will give us a good estimate of how many epochs we need to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "len(y_train_merged_am) / batch_size * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will create will behave two layers with 64 neurons each. We will train for 20 epochs and use a standard batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_model()\n",
    "history1 = model1.fit(x=X_train_merged_am, y=y_train_merged_am, epochs=EPOCHS, validation_data=(X_val_merged_am, y_val_merged_am))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try models with 2 layers of 128 nodes and 3 layers with 1 layer of 64 and 2 layers of 128 nodes. We will again train for 20 epochs and use a standard batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model([128, 128])\n",
    "history2 = model2.fit(x=X_train_merged_am, y=y_train_merged_am, epochs=EPOCHS, validation_data=(X_val_merged_am, y_val_merged_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_model([512, 256, 256])\n",
    "history3 = model3.fit(x=X_train_merged_am, y=y_train_merged_am, epochs=EPOCHS, validation_data=(X_val_merged_am, y_val_merged_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save(\"ann_merged_amusement_top_5_feat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped amusement\n",
    "\n",
    "We will train the same models as the ones we trained with the merged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "len(y_train_no_am) / batch_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_no_am = build_model()\n",
    "history1_no_am = model1_no_am.fit(x=X_train_no_am, y=y_train_no_am, epochs=EPOCHS, validation_data=(X_val_no_am, y_val_no_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_no_am = build_model([128, 128])\n",
    "history2_no_am = model2_no_am.fit(x=X_train_no_am, y=y_train_no_am, epochs=EPOCHS, validation_data=(X_val_no_am, y_val_no_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_no_am = build_model([512, 256, 256])\n",
    "history3_no_am = model3_no_am.fit(x=X_train_no_am, y=y_train_no_am, epochs=EPOCHS, validation_data=(X_val_no_am, y_val_no_am))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_no_am.save(\"ann_no_amusement_top_5_feat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "In order to compare the three models we will plot the loss and accuracy of all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation(eval_type='accuracy', histories=[], labels=[]):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(16,5))\n",
    "    for i in range(len(histories)):\n",
    "        axs[0].plot(histories[i].history[eval_type])\n",
    "        axs[1].plot(histories[i].history['val_' + eval_type])\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Epoch', ylabel=eval_type)\n",
    "        ax.legend(labels, loc='upper left')\n",
    "\n",
    "    fig.suptitle(f'Model train and validation {eval_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged amusement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [history1, history2, history3]\n",
    "labels = ['model-64,64', 'mode-128,128', 'model-512,128,128']\n",
    "plot_evaluation('accuracy', results, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('loss', results, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the model with 3 layers of 512, 128 and 128 nodes in each layer is the best model. It has a loss of ~0.15 and accuracy of ~96%. We can also see that the model is not overfitting as the loss and accuracy of the validation set are very close to the ones of the training set. Now we will plot a confusion matrix to see how the model performs on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_merged_am = model3.predict(X_val_merged_am)\n",
    "cm = confusion_matrix(y_val_merged_am, y_pred_merged_am.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy_score: {round(accuracy_score(y_val_merged_am, y_pred_merged_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"balanced_accuracy: {round(balanced_accuracy_score(y_val_merged_am, y_pred_merged_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"f1_score: {round(f1_score(y_val_merged_am, y_pred_merged_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"recall_score: {round(recall_score(y_val_merged_am, y_pred_merged_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"precision_score: {round(precision_score(y_val_merged_am, y_pred_merged_am.argmax(axis=1)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that less than 5% of the data is misclassified. Additionally, the model has way higher precission than recall, most likely because of the fact that we have more data for the baseline class than the stress class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped amusement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ = [history1_no_am, history2_no_am, history3_no_am]\n",
    "labels_ = ['model-64,64', 'mode-128,128', 'model-512,128,128']\n",
    "plot_evaluation('accuracy', results_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation('loss', results_, labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the model with 3 layers of 512, 128 and 128 nodes in each layer is the best model. It has a loss of ~0.15 and accuracy of ~94%. We can also see that the model is not overfitting as the loss and accuracy of the validation set are very close to the ones of the training set. Now we will again plot a confusion matrix to see how the model performs on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_am = model3_no_am.predict(X_val_no_am)\n",
    "cm = confusion_matrix(y_val_no_am, y_pred_no_am.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy_score: {round(accuracy_score(y_val_no_am, y_pred_no_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"balanced_accuracy: {round(balanced_accuracy_score(y_val_no_am, y_pred_no_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"f1_score: {round(f1_score(y_val_no_am, y_pred_no_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"recall_score: {round(recall_score(y_val_no_am, y_pred_no_am.argmax(axis=1)), 3)}\")\n",
    "print(f\"precision_score: {round(precision_score(y_val_no_am, y_pred_no_am.argmax(axis=1)), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that less than 5% of the data is misclassified. Additionally, in contrast with the `merged amusement` model, the `no amusement` madel has higher recall than precission. The reason for the slightly worse performance is most likely the smaller  amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainability\n",
    "\n",
    "In order to get a better understanding of how the model works we will use the SHAP library. SHAP is a game theoretic approach to explain the output of any machine learning model. SHAP values represent a feature's responsibility for a change in the model output. The sum of the SHAP values equals the difference between the expected model output and the model output when all features are set to their average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def predict_proba(self, X):\n",
    "    pred = self.predict(X).argmax(axis=1)\n",
    "    return np.array([1-pred, pred]).T\n",
    "model3.predict_proba = types.MethodType(predict_proba, model3)\n",
    "model3_no_am.predict_proba = types.MethodType(predict_proba, model3_no_am)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged amusement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_merged_am = ClassifierExplainer(model3, X_test_merged_am, y_test_merged_am)\n",
    "ExplainerDashboard(explainer_merged_am, mode=\"inline\").run(8765)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropped amusement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_no_am = ClassifierExplainer(model3_no_am, X_test_no_am, y_test_no_am)\n",
    "ExplainerDashboard(explainer_no_am, mode=\"inline\").run(8766)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df8d869fff538be8b9b2537fcc9aa2011ad88005d211355f0a8a50cbdc89c362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
